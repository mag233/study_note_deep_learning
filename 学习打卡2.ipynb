{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7d15d2",
   "metadata": {},
   "source": [
    "# 线性模型\n",
    "对应苹果书1.2章节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca52bd",
   "metadata": {},
   "source": [
    "### 关于这一章的标题和困惑\n",
    "\n",
    "我一开始看到标题写的是“线性模型”，但是翻到后面发现其实线性部分只占了很小的成分。真正的重点是把线性模型的输出再经过 Sigmoid 等激活函数，然后层层叠加，形成了一个小型的神经网络模型。对新手来说，这个跳跃有点大，因为标题会让人以为只是单纯的线性回归，但其实已经进入了神经网络的范畴。\n",
    "\n",
    "\n",
    "### 线性模型的概念\n",
    "\n",
    "线性模型的核心就是：\n",
    "y = w^T x + b\n",
    "它的本质是输入的线性加权求和，然后加上偏置。\n",
    "优点是简单、可解释性强，但缺点也很明显：只能表示“直线/超平面”的关系，表达能力有限，遇到复杂非线性问题时表现不好。\n",
    "\n",
    "### Sigmoid 函数与 ReLU 函数\n",
    "\n",
    "在神经网络里，线性输出通常要经过非线性函数变换，这个函数就叫做激活函数。\n",
    "\n",
    "`Sigmoid`：把值压缩到 (0,1)，早期常用于隐藏层和输出层。\n",
    "缺点是容易出现梯度消失，训练效率低。\n",
    "\n",
    "`ReLU` (Rectified Linear Unit)：简单高效，解决了大部分梯度消失问题，是现在主流的激活函数。\n",
    "缺点是有“死亡 ReLU”现象（某些神经元一直输出 0）。\n",
    "\n",
    "可以理解为：线性模型是骨架，激活函数是关节，能把直线骨架“弯曲”成复杂的形状。\n",
    "\n",
    "### 激活函数的概念总结\n",
    "\n",
    "激活函数的作用：\n",
    "\n",
    "- 引入非线性，让模型能逼近复杂函数。\n",
    "- 不同激活函数有不同的数值特性，影响梯度传播和训练速度。\n",
    "- 现代深度学习一般隐藏层用 ReLU 或其改进版，输出层根据任务选择 Sigmoid 或 Softmax。\n",
    "\n",
    "### 从简单模型到深层网络的优化\n",
    "\n",
    "在最早的神经网络（多层感知机 MLP）里，层数不多，主要靠 Sigmoid 激活函数叠加来提升表达力。\n",
    "后来发展出更深的网络，比如：\n",
    "\t•\tGoogLeNet (Inception v1)：22 层深度网络，提出了 Inception 模块，通过不同大小的卷积核并行操作再拼接，提升了网络的表达能力，同时减少了计算量。\n",
    "\t•\tResNet (残差网络)：解决了“深层网络难训练”的问题。核心思想是“跳跃连接（skip connection）”，即让一部分信息可以跨层直接传递，从而避免梯度消失和退化。ResNet 的出现使得神经网络层数可以轻松突破 100 层甚至上千层。\n",
    "\n",
    "这两个模型都可以看作是对早期“线性+激活函数”结构的优化和扩展：在基本单元的基础上，通过结构创新解决深度带来的训练难题。\n",
    "\n",
    "###  其他\n",
    "\n",
    "我觉得李宏毅老师的厉害之处就在于，他能从几个最基本的线性模型非常快地推导出神经网络的形式，把原理讲得很简洁。但从另一个角度看，如果读者完全没有基础，只看这本教程，很可能会在一开始感到疑惑：标题说的是线性模型，结果一翻就跳到了神经网络，这中间的过渡对新手来说跨度很大。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da72e6",
   "metadata": {},
   "source": [
    "## 概念\n",
    "\n",
    "`epoch`：一个 epoch 指的是使用全部训练数据训练一次模型。比如有 1000 个样本，batch size 是 100，那么一个 epoch 就包含 10 个 batch。\n",
    "\n",
    "`batch size`：每次训练时使用的样本数量。比如 batch size 是 32，那么每次梯度更新时会用 32 个样本计算梯度。\n",
    "\n",
    "`neuron`：神经网络中的基本计算单元，模拟生物神经元的工作方式。每个神经元接收多个输入，进行加权求和，然后通过激活函数输出结果。sigmoid 函数和 ReLU 函数是常见的激活函数，也就是神经元的非线性变换部分。\n",
    "\n",
    "`hidden layer`：神经网络中介于输入层和输出层之间的层。隐藏层的神经元通过激活函数引入非线性，使得网络能够学习复杂的模式和特征。隐藏层多的神经网络通常被称为深度神经网络（DNN）。\n",
    "\n",
    "`residual network`：残差网络（ResNet）是一种深度神经网络架构，提出了“跳跃连接”（skip connection）的概念。跳跃连接允许信息绕过一个或多个层直接传递到后面的层，从而缓解了深层网络中的梯度消失问题。ResNet 使得训练非常深的网络成为可能，并在图像识别等任务中取得了显著的效果。\n",
    "\n",
    "`overfitting`：过拟合是指模型在训练数据上表现很好，但在未见过的测试数据上表现较差的现象。通常是因为模型过于复杂，学习到了训练数据中的噪声和细节，而不是数据的整体趋势。解决过拟合的方法包括正则化、dropout、增加训练数据等。\n",
    "\n",
    "`backpropagation`：反向传播是一种用于训练神经网络的算法。它通过计算损失函数相对于每个参数的梯度，来更新网络中的权重和偏置。反向传播利用链式法则，从输出层开始，逐层向前计算梯度，直到输入层。结合梯度下降等优化算法，可以有效地训练深度神经网络。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
